# æ–‡ä»¶ï¼štabular_feature_augment.py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler
from config import config
from few_shot.swav_anchor import SwAVAnchorExtractor
import torch
import torch.nn.functional as F


def augment_positive_samples(df, n_aug=1, dist_thresh=2.5, visualize=True, max_total=None):
    """
    åŸºäºè¡¨æ ¼ç‰¹å¾å¯¹æ­£ç±»æ ·æœ¬è¿›è¡Œå¢å¼ºï¼š
    - df åŒ…å« 'label', 'image_filename' ä»¥åŠ config['SELECTED_COLS'] ä¸­çš„åˆ—
    - n_aug: æ¯å¯¹æ ·æœ¬ç”Ÿæˆçš„å¢å¼ºæ ·æœ¬æ•°
    - dist_thresh: åªå¯¹è·ç¦» <= é˜ˆå€¼çš„å¯¹è¿›è¡Œå¢å¼º
    - visualize: æ˜¯å¦ä¿å­˜è·ç¦»ç›´æ–¹å›¾
    - max_total: å¢å¼ºåæ­£ç±»æ ·æœ¬çš„æœ€å¤§æ€»æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    è¿”å›æ‰©å¢åçš„ DataFrameï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆå¯¹åˆ™ç›´æ¥è¿”å›åŸ dfã€‚
    """
    pos_df = df[df['label'] == 1].reset_index(drop=True)
    neg_df = df[df['label'] == 0].reset_index(drop=True)
    
    # å¦‚æœæ­£ç±»æ ·æœ¬æ•°é‡å·²ç»å¾ˆå¤šï¼Œå¯ä»¥è€ƒè™‘ä¸å¢å¼º
    if max_total and len(pos_df) >= max_total:
        print(f"æ­£ç±»æ ·æœ¬æ•°é‡({len(pos_df)})å·²è¾¾åˆ°è®¾å®šä¸Šé™({max_total})ï¼Œè·³è¿‡å¢å¼º")
        return df
        
    if len(pos_df) < 2:
        print("Warning: not enough positive samples to augment.")
        return df

    # å‡†å¤‡åŸå§‹ç‰¹å¾çŸ©é˜µ
    selected_cols = config['SELECTED_COLS']
    X_raw = pos_df[selected_cols].astype(np.float32).values

    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X = scaler.fit_transform(X_raw)

    # è®¡ç®—ä¸¤ä¸¤è·ç¦»ï¼Œä»…å–ä¸Šä¸‰è§’
    dist_matrix = pairwise_distances(X, metric='euclidean')
    idxs = np.triu_indices_from(dist_matrix, k=1)
    dist_vals = dist_matrix[idxs]

    # è¿‡æ»¤ NaN
    valid = dist_vals[~np.isnan(dist_vals)]
    if valid.size == 0:
        print("Warning: no valid distances, skip augmentation.")
        return df

    # æ‰“å°è·ç¦»èŒƒå›´
    print(f"Distance range among positive samples: min={valid.min():.3f}, max={valid.max():.3f}")

    # å¯è§†åŒ–è·ç¦»åˆ†å¸ƒ
    if visualize:
        os.makedirs(config['RESULT_PATH'], exist_ok=True)
        plt.figure()
        plt.hist(valid, bins=30, edgecolor='black')
        plt.title("Positiveâ€“Positive Distance Distribution")
        plt.savefig(os.path.join(config['RESULT_PATH'], 'pp_distance_hist.png'))
        plt.close()

    # ä¼˜åŒ–ï¼šä½¿ç”¨è‡ªé€‚åº”è·ç¦»é˜ˆå€¼
    if dist_thresh == 'auto':
        # è‡ªåŠ¨é€‰æ‹©è·ç¦»é˜ˆå€¼ä¸ºè·ç¦»åˆ†å¸ƒçš„25%åˆ†ä½æ•°
        dist_thresh = np.percentile(valid, 25)
        print(f"è‡ªåŠ¨é€‰æ‹©è·ç¦»é˜ˆå€¼: {dist_thresh:.3f}")
    
    # ç­›é€‰è·ç¦»å°äºç­‰äºé˜ˆå€¼çš„å¯¹
    mask = valid <= dist_thresh
    if not mask.any():
        print(f"Warning: no pairs under dist_thresh={dist_thresh}, skip augmentation.")
        return df
    pairs = list(zip(idxs[0][mask], idxs[1][mask]))
    
    # å¯¹äºå°æ ·æœ¬å­¦ä¹ ï¼Œé™åˆ¶ç”Ÿæˆçš„å¯¹æ•°
    max_pairs = min(len(pairs), 50)  # æœ€å¤šä½¿ç”¨50å¯¹æ ·æœ¬è¿›è¡Œå¢å¼º
    pairs = pairs[:max_pairs]
    print(f"ä½¿ç”¨ {len(pairs)} å¯¹æ ·æœ¬è¿›è¡Œå¢å¼º (ä» {len(mask[mask])} å¯¹ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ä¸­é€‰æ‹©)")

    # çº¿æ€§æ’å€¼ç”Ÿæˆå¢å¼ºæ ·æœ¬
    aug_rows = []
    for i, j in pairs:
        for _ in range(n_aug):
            alpha = np.random.rand()
            new_feat = alpha * X_raw[i] + (1 - alpha) * X_raw[j]
            row = pos_df.iloc[i].copy()
            for k, col in enumerate(selected_cols):
                row[col] = float(new_feat[k])
            aug_rows.append(row)

    # åœ¨ç”Ÿæˆå¢å¼ºæ ·æœ¬åï¼Œæ£€æŸ¥æ€»æ•°æ˜¯å¦è¶…è¿‡ä¸Šé™
    if max_total and len(pos_df) + len(aug_rows) > max_total:
        # åªå–éƒ¨åˆ†å¢å¼ºæ ·æœ¬ï¼Œä½¿æ€»æ•°ä¸è¶…è¿‡ä¸Šé™
        samples_to_take = max_total - len(pos_df)
        if samples_to_take > 0:
            aug_rows = aug_rows[:samples_to_take]
        else:
            aug_rows = []
    
    # åˆå¹¶å¢å¼ºæ ·æœ¬
    if not aug_rows:
        return df
        
    aug_df = pd.DataFrame(aug_rows)
    aug_df['label'] = 1
    aug_df['image_filename'] = np.random.choice(pos_df['image_filename'], size=len(aug_df))

    df_new = pd.concat([neg_df, pos_df, aug_df], ignore_index=True)
    print(f"ğŸ” æ­£ç±»æ ·æœ¬æ‰©å¢ï¼šåŸå§‹ {len(pos_df)} â†’ å¢å¼ºå {len(aug_df)} â†’ æ€»å…± {len(df_new[df_new['label']==1])}")
    return df_new

# æ·»åŠ è´Ÿç±»æ ·æœ¬å¢å¼ºå‡½æ•°
def augment_negative_samples(df, target_count, selected_cols, dist_thresh=2.5, visualize=True):
    """
    åŸºäºè¡¨æ ¼ç‰¹å¾å¯¹è´Ÿç±»æ ·æœ¬è¿›è¡Œå¢å¼ºï¼š
    - df åŒ…å« 'label', 'image_filename' ä»¥åŠ selected_cols ä¸­çš„åˆ—
    - target_count: ç›®æ ‡è´Ÿç±»æ ·æœ¬æ•°é‡
    - selected_cols: ç”¨äºå¢å¼ºçš„ç‰¹å¾åˆ—
    - dist_thresh: åªå¯¹è·ç¦» <= é˜ˆå€¼çš„å¯¹è¿›è¡Œå¢å¼º
    - visualize: æ˜¯å¦ä¿å­˜è·ç¦»ç›´æ–¹å›¾
    è¿”å›æ‰©å¢åçš„ DataFrameï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆå¯¹åˆ™ç›´æ¥è¿”å›åŸ dfã€‚
    """
    neg_df = df[df['label'] == 0].reset_index(drop=True)
    pos_df = df[df['label'] == 1].reset_index(drop=True)
    
    # å¦‚æœè´Ÿç±»æ ·æœ¬å·²ç»è¶³å¤Ÿï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®
    if len(neg_df) >= target_count:
        return df
    
    if len(neg_df) < 2:
        print("Warning: not enough negative samples to augment.")
        return df

    # å‡†å¤‡åŸå§‹ç‰¹å¾çŸ©é˜µ
    X_raw = neg_df[selected_cols].astype(np.float32).values

    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X = scaler.fit_transform(X_raw)

    # è®¡ç®—ä¸¤ä¸¤è·ç¦»ï¼Œä»…å–ä¸Šä¸‰è§’
    dist_matrix = pairwise_distances(X, metric='euclidean')
    idxs = np.triu_indices_from(dist_matrix, k=1)
    dist_vals = dist_matrix[idxs]

    # è¿‡æ»¤ NaN
    valid = dist_vals[~np.isnan(dist_vals)]
    if valid.size == 0:
        print("Warning: no valid distances, skip augmentation.")
        return df

    # æ‰“å°è·ç¦»èŒƒå›´
    print(f"Distance range among negative samples: min={valid.min():.3f}, max={valid.max():.3f}")

    # å¯è§†åŒ–è·ç¦»åˆ†å¸ƒ
    if visualize:
        os.makedirs(config['RESULT_PATH'], exist_ok=True)
        plt.figure()
        plt.hist(valid, bins=30, edgecolor='black')
        plt.title("Negativeâ€“Negative Distance Distribution")
        plt.savefig(os.path.join(config['RESULT_PATH'], 'nn_distance_hist.png'))
        plt.close()

    # è‡ªåŠ¨é€‰æ‹©è·ç¦»é˜ˆå€¼ä¸ºè·ç¦»åˆ†å¸ƒçš„25%åˆ†ä½æ•°
    if dist_thresh == 'auto':
        dist_thresh = np.percentile(valid, 25)
        print(f"è‡ªåŠ¨é€‰æ‹©è·ç¦»é˜ˆå€¼: {dist_thresh:.3f}")

    # ç­›é€‰è·ç¦»å°äºç­‰äºé˜ˆå€¼çš„å¯¹
    mask = valid <= dist_thresh
    if not mask.any():
        print(f"Warning: no pairs under dist_thresh={dist_thresh}, skip augmentation.")
        return df
    pairs = list(zip(idxs[0][mask], idxs[1][mask]))
    
    # å¯¹äºå°æ ·æœ¬å­¦ä¹ ï¼Œé™åˆ¶ç”Ÿæˆçš„å¯¹æ•°
    max_pairs = min(len(pairs), 50)  # æœ€å¤šä½¿ç”¨50å¯¹æ ·æœ¬è¿›è¡Œå¢å¼º
    pairs = pairs[:max_pairs]
    print(f"ä½¿ç”¨ {len(pairs)} å¯¹æ ·æœ¬è¿›è¡Œå¢å¼º (ä» {len(mask[mask])} å¯¹ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ä¸­é€‰æ‹©)")

    # è®¡ç®—éœ€è¦çš„å¢å¼ºæ ·æœ¬æ•°é‡
    samples_needed = target_count - len(neg_df)
    n_aug = max(1, int(np.ceil(samples_needed / len(pairs))))
    
    # çº¿æ€§æ’å€¼ç”Ÿæˆå¢å¼ºæ ·æœ¬
    aug_rows = []
    for i, j in pairs:
        for _ in range(n_aug):
            alpha = np.random.rand() * 0.8 + 0.1  # é™åˆ¶åœ¨0.1-0.9ä¹‹é—´
            new_feat = alpha * X_raw[i] + (1 - alpha) * X_raw[j]
            row = neg_df.iloc[i].copy()
            for k, col in enumerate(selected_cols):
                row[col] = float(new_feat[k])
            aug_rows.append(row)
            
            # å¦‚æœå·²ç»ç”Ÿæˆè¶³å¤Ÿçš„æ ·æœ¬ï¼Œå°±åœæ­¢
            if len(aug_rows) >= samples_needed:
                break
        if len(aug_rows) >= samples_needed:
            break

    # åˆå¹¶å¢å¼ºæ ·æœ¬
    if not aug_rows:
        return df
        
    aug_df = pd.DataFrame(aug_rows)
    aug_df['label'] = 0  # ç¡®ä¿æ ‡ç­¾ä¸ºè´Ÿç±»
    
    # ç¡®ä¿image_filenameåˆ—å­˜åœ¨
    if 'image_filename' in neg_df.columns:
        aug_df['image_filename'] = np.random.choice(neg_df['image_filename'], size=len(aug_df))

    df_new = pd.concat([pos_df, neg_df, aug_df], ignore_index=True)
    print(f"ğŸ” è´Ÿç±»æ ·æœ¬æ‰©å¢ï¼šåŸå§‹ {len(neg_df)} â†’ å¢å¼ºå {len(aug_df)} â†’ æ€»å…± {len(df_new[df_new['label']==0])}")
    return df_new




